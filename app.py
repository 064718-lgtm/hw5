import re
import statistics
from typing import Dict, List, Tuple

import altair as alt
import pandas as pd
import streamlit as st

# Basic stopword list to support the heuristic.
STOPWORDS = {
    "a",
    "an",
    "and",
    "are",
    "as",
    "at",
    "be",
    "but",
    "by",
    "for",
    "from",
    "has",
    "have",
    "if",
    "in",
    "is",
    "it",
    "of",
    "on",
    "or",
    "that",
    "the",
    "this",
    "to",
    "was",
    "were",
    "will",
    "with",
}

def clamp(value: float, low: float = 0.0, high: float = 1.0) -> float:
    return max(low, min(high, value))


def tokenize(text: str) -> List[str]:
    return re.findall(r"[A-Za-z']+", text.lower())


def token_weights(text: str) -> pd.DataFrame:
    tokens = tokenize(text)
    total = len(tokens)
    data = []
    if total == 0:
        return pd.DataFrame(columns=["token", "count", "weight"])

    counts: Dict[str, int] = {}
    for tok in tokens:
        counts[tok] = counts.get(tok, 0) + 1

    for tok, count in counts.items():
        data.append({"token": tok, "count": count, "weight": count / total})

    df = pd.DataFrame(data).sort_values(by=["weight", "token"], ascending=[False, True])
    return df


def heuristic_features(text: str) -> Dict[str, float]:
    tokens = tokenize(text)
    num_tokens = len(tokens)
    unique_ratio = len(set(tokens)) / num_tokens if num_tokens else 0.0
    stopword_hits = sum(1 for token in tokens if token in STOPWORDS)
    stop_ratio = stopword_hits / num_tokens if num_tokens else 0.0

    sentences = [segment.strip() for segment in re.split(r"[.!?]+", text) if segment.strip()]
    sentence_lengths = [len(tokenize(sentence)) for sentence in sentences] or [0]
    avg_len = statistics.mean(sentence_lengths)
    length_std = statistics.stdev(sentence_lengths) if len(sentence_lengths) > 1 else 0.0

    return {
        "tokens": num_tokens,
        "unique_ratio": unique_ratio,
        "stop_ratio": stop_ratio,
        "avg_sentence_len": avg_len,
        "sentence_len_std": length_std,
    }


def heuristic_score(text: str) -> Tuple[float, Dict[str, float]]:
    features = heuristic_features(text)
    if features["tokens"] == 0:
        return 0.5, features

    # AI-like texts tend to have balanced vocabulary diversity, smoother sentence lengths,
    # and moderate stopword ratios. Bias the heuristic toward those patterns.
    unique_balance = 1.0 - clamp(abs(features["unique_ratio"] - 0.62) / 0.62, 0.0, 1.0)
    smoothness_score = clamp((12.0 - features["sentence_len_std"]) / 12.0, 0.0, 1.0)
    medium_length_score = clamp(1.0 - abs(features["avg_sentence_len"] - 18.0) / 18.0, 0.0, 1.0)
    stopword_balance = 1.0 - clamp(abs(features["stop_ratio"] - 0.46) / 0.46, 0.0, 1.0)

    weighted_score = (
        0.3 * unique_balance
        + 0.25 * smoothness_score
        + 0.25 * stopword_balance
        + 0.2 * medium_length_score
    )
    return clamp(weighted_score, 0.0, 1.0), features


def analyze(text: str) -> Dict:
    ai_prob, features = heuristic_score(text)
    return {
        "ai_prob": ai_prob,
        "human_prob": 1.0 - ai_prob,
        "method": "heuristic",
        "meta": {"features": features},
    }


st.set_page_config(page_title="AI / Human Detector", page_icon="ğŸ›°ï¸", layout="wide")
st.title("AI / Human æ–‡ç« åµæ¸¬å™¨")
st.caption("è¼¸å…¥æ–‡æœ¬å¾ŒæŒ‰ä¸‹åˆ†æï¼Œä¼°è¨ˆ AI vs Human çš„æ¯”ä¾‹ã€‚æ­¤ç‰ˆæœ¬æ¡ç”¨å•Ÿç™¼å¼ç‰¹å¾µã€‚")

with st.sidebar:
    st.subheader("è¨­å®š")
    sample_choice = st.selectbox(
        "ç¯„ä¾‹æ–‡æœ¬",
        ["-- ä¸ä½¿ç”¨ç¯„ä¾‹ --", "AI-like sample", "Human-like sample"],
        index=0,
    )
    st.markdown(
        """
        - å®‰è£éœ€æ±‚ï¼š`pip install -r requirements.txt`
        - å•Ÿå‹•ï¼š`streamlit run app.py`
        """
    )

default_text = (
    "Artificial intelligence systems can produce fluent and well-structured prose. "
    "This detector estimates how likely the passage was generated by a language model "
    "compared with a human author."
)

if "input_text" not in st.session_state:
    st.session_state["input_text"] = default_text

prev_choice = st.session_state.get("sample_choice", "-- ä¸ä½¿ç”¨ç¯„ä¾‹ --")
if sample_choice != prev_choice:
    if sample_choice == "AI-like sample":
        st.session_state["input_text"] = (
            "As an AI language model, I can provide guidance, structure, and coherent paragraphs "
            "that align with the requested topic while maintaining a neutral tone throughout the response."
        )
    elif sample_choice == "Human-like sample":
        st.session_state["input_text"] = (
            "I remember walking past the old bookstore on my way home and pausing to read the hand-written sign "
            "in the window. It wasn't polished, but the crooked letters felt sincere, and that small detail "
            "stuck with me more than the novel I eventually bought."
        )
    else:
        st.session_state["input_text"] = default_text
st.session_state["sample_choice"] = sample_choice

with st.form("detector_form", clear_on_submit=False):
    text = st.text_area(
        "è¼¸å…¥æˆ–è²¼ä¸Šæ–‡æœ¬ï¼ˆè‹±æ–‡èˆ‡ä¸­è‹±æ··æ’çš†å¯ï¼‰",
        key="input_text",
        height=260,
    )
    submitted = st.form_submit_button("åˆ†æ Analyze", type="primary")

if submitted and text.strip():
    with st.spinner("Analyzing..."):
        result = analyze(text.strip())

    ai_pct = result["ai_prob"] * 100
    human_pct = result["human_prob"] * 100
    col1, col2 = st.columns(2)
    col1.metric("AI æ©Ÿç‡", f"{ai_pct:.1f}%")
    col2.metric("Human æ©Ÿç‡", f"{human_pct:.1f}%")

    st.bar_chart(
        pd.DataFrame({"probability": [ai_pct, human_pct]}, index=["AI", "Human"])
    )

    with st.expander("ç‰¹å¾µæ‘˜è¦", expanded=False):
        feats = result["meta"].get("features", {})
        st.write(
            f"Tokens: {feats.get('tokens', 0)}, "
            f"Unique ratio: {feats.get('unique_ratio', 0.0):.3f}, "
            f"Stopword ratio: {feats.get('stop_ratio', 0.0):.3f}"
        )
        st.write(
            f"Avg sentence length: {feats.get('avg_sentence_len', 0.0):.2f}, "
            f"Sentence length std: {feats.get('sentence_len_std', 0.0):.2f}"
        )

    # Token weight visualization
    st.subheader("Token æ¬Šé‡è¦–è¦ºåŒ–")
    token_df = token_weights(text)
    if not token_df.empty:
        max_tokens = max(1, min(50, len(token_df)))
        default_n = min(15, max_tokens)
        top_n = st.slider(
            "é¡¯ç¤ºå‰ N å€‹æ¬Šé‡è¼ƒé«˜çš„ token",
            min_value=1,
            max_value=max_tokens,
            value=default_n,
        )
        top_tokens = token_df.head(top_n)
        chart = (
            alt.Chart(top_tokens)
            .mark_bar(cornerRadiusTopLeft=3, cornerRadiusTopRight=3)
            .encode(
                x=alt.X("weight:Q", title="æ¬Šé‡ (é »ç‡ä½”æ¯”)", axis=alt.Axis(format="~%")),
                y=alt.Y("token:N", sort="-x", title="Token"),
                color=alt.value("#4C78A8"),
                tooltip=["token", "count", alt.Tooltip("weight:Q", format=".3f")],
            )
            .properties(height=400)
        )
        st.altair_chart(chart, use_container_width=True)
        st.caption("æ¬Šé‡ = token å‡ºç¾æ¬¡æ•¸ / ç¸½ token æ•¸ï¼Œä¾¿æ–¼è§€å¯Ÿé«˜é »è©å°åˆ¤æ–·çš„å½±éŸ¿ã€‚")
    else:
        st.info("å°šç„¡å¯è¦–è¦ºåŒ–çš„ tokenï¼ˆè«‹è¼¸å…¥æ–‡æœ¬ï¼‰ã€‚")

    if result["meta"].get("detector_error"):
        st.info(result["meta"]["detector_error"])
elif submitted:
    st.warning("è«‹è¼¸å…¥æ–‡æœ¬å†é€²è¡Œåˆ†æã€‚")
